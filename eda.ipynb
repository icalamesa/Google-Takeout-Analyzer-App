{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAKEOUT_PATH = \"/home/ivan/Desktop/datasets/Takeout\" # CHANGE IF NECESSARY\n",
    "ACTIVITY_LOG_PATH = os.path.join(TAKEOUT_PATH, \"Mi actividad\")\n",
    "\n",
    "ACTIVITY_PLACEHOLDER_NAME = \"MiActividad.html\"\n",
    "\n",
    "#drive\n",
    "FOLDER_DRIVE_ACTIVITY_LOG_PATH = os.path.join(ACTIVITY_LOG_PATH, \"Drive\")\n",
    "FILE_DRIVE_ACTIVITY_LOG_PATH = os.path.join(FOLDER_DRIVE_ACTIVITY_LOG_PATH, ACTIVITY_PLACEHOLDER_NAME)\n",
    "\n",
    "#takeout\n",
    "FOLDER_DRIVE_ACTIVITY_LOG_PATH = os.path.join(ACTIVITY_LOG_PATH, \"Takeout\")\n",
    "FILE_TAKEOUT_ACTIVITY_LOG_PATH = os.path.join(FOLDER_DRIVE_ACTIVITY_LOG_PATH, ACTIVITY_PLACEHOLDER_NAME)\n",
    "\n",
    "#youtube\n",
    "FOLDER_DRIVE_ACTIVITY_LOG_PATH = os.path.join(ACTIVITY_LOG_PATH, \"YouTube\")\n",
    "FILE_YOUTUBE_ACTIVITY_LOG_PATH = os.path.join(FOLDER_DRIVE_ACTIVITY_LOG_PATH, ACTIVITY_PLACEHOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_html_contents(file_path, chunk_size=524288):\n",
    "    \"\"\"\n",
    "    Generator function that reads an HTML file in chunks to avoid loading the entire file into memory.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the HTML file.\n",
    "    - chunk_size (int): Size of each chunk in bytes. Default is 512 KB.\n",
    "    \n",
    "    Yields:\n",
    "    - str: A chunk of HTML that ends on a complete tag.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        buffer = ''\n",
    "        while True:\n",
    "            data = file.read(chunk_size)\n",
    "            if not data:\n",
    "                # Only yield the remaining buffer if it's a valid chunk with a closed tag\n",
    "                if buffer:\n",
    "                    yield buffer\n",
    "                break\n",
    "\n",
    "            buffer += data\n",
    "            last_tag_end = max(buffer.rfind('>'), buffer.rfind('/>'))\n",
    "            if last_tag_end == -1:\n",
    "                continue  # Continue reading into buffer until a tag end is found\n",
    "\n",
    "            # Yield up to the last complete tag and adjust the buffer\n",
    "            yield buffer[:last_tag_end + 1]\n",
    "            buffer = buffer[last_tag_end + 1:]\n",
    "\n",
    "\n",
    "def process_chunk(html_content):\n",
    "    \"\"\"\n",
    "    Processes a chunk of HTML to extract relevant data from specified div elements, including a timestamp.\n",
    "    \n",
    "    Args:\n",
    "    - html_content (str): A string of HTML content.\n",
    "    \n",
    "    Returns:\n",
    "    - list of dict: Extracted data from each content cell in the HTML chunk.\n",
    "    \"\"\"\n",
    "    strainer = SoupStrainer('div', class_=\"outer-cell mdl-cell mdl-cell--12-col mdl-shadow--2dp\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser', parse_only=strainer)\n",
    "    entries = []\n",
    "    \n",
    "    for outer_div in soup.find_all('div'):\n",
    "        content_cells = outer_div.find_all('div', class_=\"content-cell mdl-cell mdl-cell--6-col mdl-typography--body-1\")\n",
    "        for div in content_cells:\n",
    "            links = div.find_all('a', href=True)\n",
    "            complete_action = ' '.join(div.stripped_strings)\n",
    "            action_code = complete_action.split('|')[0].strip() if '|' in complete_action else complete_action\n",
    "            timestamp = complete_action.split('|')[-1].strip() if '|' in complete_action else ''\n",
    "            entry = {\n",
    "                \"complete_action\": complete_action,\n",
    "                \"action_code\": action_code,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"link_action_name\": links[0]['href'] if len(links) > 0 else '',\n",
    "                \"link_action_text\": links[0].get_text(strip=True) if len(links) > 0 else '',\n",
    "                \"channel_link\": links[1]['href'] if len(links) > 1 else '',\n",
    "                \"channel_name\": links[1].get_text(strip=True) if len(links) > 1 else '',\n",
    "                \"link3\": links[2]['href'] if len(links) > 2 else '',\n",
    "                \"link3_text\": links[2].get_text(strip=True) if len(links) > 2 else '',\n",
    "            }\n",
    "            entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def test(file_path):\n",
    "    \"\"\"\n",
    "    Main function to process an HTML file, extract data, and write to a CSV file using multiprocessing.\n",
    "    Dynamically adjusts the number of processes based on the file size.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the HTML file.\n",
    "    \"\"\"\n",
    "    # Determine file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    chunk_size = 524288  # 512 KB\n",
    "    num_chunks = (file_size // chunk_size) + 1 \n",
    "    \n",
    "    if num_chunks < 4:\n",
    "        num_processes = num_chunks\n",
    "    else:\n",
    "        num_processes = 4\n",
    "    \n",
    "    # Read chunks and process them\n",
    "    chunks = get_html_contents(file_path, chunk_size=chunk_size)\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        result_iter = pool.imap(process_chunk, chunks)\n",
    "        results = list(itertools.chain.from_iterable(result_iter))\n",
    "    \n",
    "    # Convert results to a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('output.csv', index=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "channel_name\n",
       "    4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = test(FILE_DRIVE_ACTIVITY_LOG_PATH)\n",
    "print(len(df))\n",
    "value_counts = df[\"channel_name\"].value_counts().sort_values(ascending=False)    \n",
    "value_counts.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
